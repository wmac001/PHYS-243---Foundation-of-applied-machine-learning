{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain Monte Carlo simulation with an example for it's application. (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte carlo simulations are used to model the behavior of complex systems due to random variables. It is used to understand uncertainty/risk in prediction models. It can also be use to find parameters of a given model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How can you sample from a given probability density function pdf? (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to sample from the cumulative distribution function (CDF) for the given PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: (25 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain K-means clustering, Hierarchical clustering and explain a scenario in which\n",
    "### you prefer Hierarchical to K-means and vice versa. (10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is great if your dataset is small and do not have labels. \n",
    "\n",
    "It works by starting with starting with your dataset as one big cluster. After this, it takes the most similar elements again and puts them in another cluster. It keeps repeating until every single tuple has it's own cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is much faster, but only works with unlabeled data. \n",
    "\n",
    "1) It works by taking an input of K as the # of centroids we wish to start out with. \n",
    "    Initially the centroids are random points from the input dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) Then it assigns the nearest tuple to each of the centroids in respect to distance and updates the centroid by taking the average of all objects(so far 2) in the cluster. \n",
    "    Now the centroid is no longer a tuple existing from the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3) #2 is repeated until all tuples are classified into the k centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- Both have their advantages. Both only work for unlabeled data. K-means clustering is good if your dataset is very very large. Hierarchical works well if your dataset is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain K-Nearest Neighbors and how would you make decision on what K to choose.(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a classification technique that works on labeled data. It classifies based on distance. It takes in a parameter K for the number of nearest neighbors. The dataset is then trained. Then k is then used to classify the test data set to your trained data you wish to allow to be classified. The label(s) that occur most frequently (appear the closest) to the trained data are taken to be k neighbors.\n",
    "\n",
    "In order to decide which k to use, the best way is to use a recursive algorithm to find the right k to use. I.E a for loop with k 1 - 10. \n",
    "\n",
    "The user must keep in mind that by increasing k, we are increasing the error in our predictions. In most cases, k should be very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explain Decision Trees, and Random Forest.(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier is like a flowchart. It has root nodes and branches into leaf nodes. The input data is split and classified. To maximize the decision tree classifier, we need to maximize the gini gain. It will continue splitting until the max gini gain is reached or if the depth parameter is reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a collection of decision trees based on bootstrap sampling. The features used to generate the initial trees are chosen at random. Classification occurs by taking the average of all the decision trees. Random forest is used to adress the issue of overfitting with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain Kernel methods, Support Vector Machines and explain the difference between linear and Gaussian kernels. (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel methods are various algorithms in adjusting the hyperplane used for classification. This method can be applied on a wide range of various methods. \n",
    "\n",
    "There are four kernel method: Linear, Gaussian (RBF), Poly and Sigmoid.\n",
    "\n",
    "\n",
    "SVM is a classification method and it works by fitting a boundary(s) to a region of points that are all similar on a training sample. New points from the test sample are classified based on whether they fall on either side of this line. The support vectors are named this way because each point is vector and they are supporting the boundary(hyperplane).\n",
    "\n",
    "\n",
    "\n",
    "Linear is used when the data is linearly separable using a single line.\n",
    "\n",
    "Gaussian is a weighted linear combination of the kernel function a data point and each support vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain ways you can evaluate the performance of a certain model. (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can evaluate the performance on a certain model in various ways. You can do it by running a recursive algoritm for various parameters, then finding the ratio of predicted values/actual values. Or if the algorithm is very heavy on computational power like SVM is, you will have to run the algorithm and tune the parameters step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain over\f",
    "tting and how can you avoid that. (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is when your model is learning too much noise from your data. This negatively impacts the performance of the model on new data. This can occur by having parameters that are too far off. I.E. too much learning rate, too high degree of freedom. \n",
    "\n",
    "You can avoid this by taking a visual plot of your results after running multiple algorithms of different input parameters, use a resampling technique like cross-validation for model accuracy and hold back a validation dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explain cross-validation. (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling technique used to effectiveness of a machine learning model. The lowest validation set error is chosen. Generally about 10% of the dataset is used for cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: (50 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagine that you have a dataset (100 features, 1000 instances) with both categorical and real valued data and they have been labeled with a categorical value. Also, your dataset contains few missing data. Your task here is to classify the labels. Explain with details the procedures you take to find a classifier with two different algorithms of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First I will bring in the dataset.\n",
    "\n",
    "2) Then I will run a correlation analysis on the dataset to see which of the 100 features I will use.\n",
    "\n",
    "3) If the dataset is missing alot of data for a given feature, I will remove that feature. Else, I will just delete the instances with missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) I will then normalize the dataset into numbers 0,1,2,3 etc for each feature depending on what their values are. I will also normalize the labels as well into numbers 0,1,2,3 depending on what available categorical value is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) I will then split the dataset into training, testing splits. Then split the training into training and validation splits.\n",
    "\n",
    "70/30 split for training/testing and then a 80/20 split for training/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Using my training data, I will then run a random forest recursive algorithm for the level of depth to maximize accuracy and score my model for each loop using the validation data set to create my random forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Then I will fit my test data set using my random forest model and compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) After Random Forest testing is done, I will use my training data to run a SVM recursive algorithm for various gamma, cost, and kernels(linear, rbf, poly, sigmoid) to get the best accuracy and score my model for each loop using the validation data set to create my SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Then I will fit my test data set using my SVM model and compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) I will chose the model with the highest test accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
